
---

## Docker Limitations

* **No self-healing:** If a container crashes, Docker does not automatically restart or replace it, making it less reliable.
* **Volume management:** Docker lacks advanced or proper volume management features.
* **Network connectivity:** Networking capabilities are limited and not robust for complex scenarios.
* **No auto-scaling:** Docker alone cannot scale containers based on demand.
* **Secrets and configuration management:** Docker does not provide built-in strong secrets or config management.

Currently, we build container images with Docker but run them using Kubernetes (K8s). Kubernetes is responsible for running containers, managing network connectivity, handling volume mounts, and ensuring better orchestration and scalability.

---

## Kubernetes Architecture Overview

Kubernetes consists of:

* **Workstation:** Where you interact with the cluster (e.g., using `kubectl`).
* **Control Plane / Master Node:** Manages the cluster state.
* **Worker Nodes:** Where containers (pods) run.

---

### Control Plane Components

1. **kube-apiserver**
   The API server is the frontend of Kubernetes‚Äô control plane. It handles all RESTful API requests from users and components, validates and authenticates them, and updates the cluster state stored in etcd.

2. **etcd**
   A distributed key-value store that holds all cluster data ‚Äî the ‚Äúsource of truth‚Äù for Kubernetes. It stores configuration, state data like nodes, pods, and ConfigMaps.

3. **kube-scheduler**
   Watches for pods that don‚Äôt have an assigned node yet. It assigns pods to nodes based on resource availability, affinity rules, taints, and tolerations.

4. **kube-controller-manager**
   Runs controllers that maintain the cluster‚Äôs desired state by monitoring and reconciling it. Examples:

   * **Node Controller:** Checks node health
   * **Replication Controller:** Ensures the desired number of pod replicas
   * **Job Controller:** Manages batch jobs
   * **Endpoints Controller:** Manages network endpoints

5. **cloud-controller-manager**
   Integrates Kubernetes with the cloud provider (AWS, GCP, Azure). It manages cloud-specific tasks such as load balancers, storage volumes, and node lifecycle.

---

### Node-Level Components

1. **kubelet**
   An agent running on each worker node. It receives pod specs from the API server and ensures the containers are running and healthy on its node.

2. **kube-proxy**
   Manages network routing on each node. It configures network rules to route traffic to the correct pods, enabling communication between services.

3. **Container Runtime**
   The software responsible for running containers on nodes, e.g., Docker, containerd, or CRI-O. The kubelet uses the runtime to start, stop, and manage containers.

---

## Workstation: Tools for Managing Kubernetes on AWS EKS

### 1. **eksctl**

`eksctl` is a simple CLI tool used to create and manage Kubernetes clusters on Amazon EKS. It automates many cluster setup tasks and simplifies cluster management.

* **Installation:** Follow the instructions at [eksctl installation](https://eksctl.io/installation/).

* **Sample cluster config (eks.yml):**

```yaml
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: expense
  region: us-east-1

managedNodeGroups:
- name: expense
  instanceType: m5.large
  desiredCapacity: 3
  spot: true
```

* **Commands:**

```bash
eksctl create cluster --config-file=<path-to-eks.yml>   # Create an EKS cluster
eksctl delete cluster --config-file=<path-to-eks.yml>   # Delete the EKS cluster
```

---

### 2. **kubectl**

`kubectl` is the Kubernetes CLI tool used to interact with and manage Kubernetes clusters, pods, services, and other resources.

* **Installation:** Follow instructions at [AWS EKS kubectl guide](https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html).

* **Common commands:**

```bash
kubectl api-resources                  # List all available Kubernetes resource types

kubectl get nodes                      # Display all nodes in the cluster

kubectl create namespace <name>       # Create a new namespace

kubectl delete namespace <name>       # Delete an existing namespace

kubectl describe <resource> <name>    # Show detailed info about a specific resource

kubectl get ns                        # Shortcut to list all namespaces
```

---

## k8s-resources ##
---

### 1. **Namespace**

A **Namespace** is a way to **divide a Kubernetes cluster into isolated environments**. Think of it as a separate workspace where you can group related resources like pods, services, and deployments.

---

‚úÖ **Why we use Namespaces**:

* To **separate teams or projects** within the same cluster
* To apply **different access controls, quotas, or policies**
* To avoid **name conflicts** across environments (e.g., `dev`, `test`, `prod`)

---

üìå **Best Practices**:

* Use namespaces for logical separation of concerns
* Apply **resource limits and RBAC rules** per namespace
* Avoid using the default namespace for production workloads

example manifest file to create namespace
```bash
Kind: Namespace
apiVersion: v1
metadata:
    name: project
    labels:
        name: project
        environment: dev
```

kubectl apply -f <file.yml> # to create k8s resources

kubectl delete -f <file.yml> # to delete k8s resources

---

### 2. **Pod**

A **Pod** is the **smallest deployable unit** in Kubernetes. It represents a single instance of a running process in a cluster.

---

‚úÖ **Key Features**:

* A pod typically runs **one container**, but it can run multiple if needed
* All containers in a pod share the **same network IP and storage**
* Pods are **short-lived** and usually managed by controllers like Deployments

---

üìå **Why we use Pods**:

* To host application containers
* To group containers that must share context (e.g., logging agents with main app)
* To serve as the building block for Kubernetes workloads

example manifest file to create pod
```bash
Kind: pod
apiVersion: v1
metadata: 
    name: nginx
spec:
    containers:
    - name: nginx
      image: nginx
```

kubectl apply -f <pod.yml>

kubectl describe pod <pod-name> # to see pod info

---

### 3. **Multi-Container Pod**

A **multi-container pod** runs two or more containers in the same pod.These containers work together and can communicate over `localhost`.

---

‚úÖ **Why we use Multi-Container Pods**:

* To create **sidecar containers** for logging, monitoring, or proxying
* To run **helper processes** alongside the main application
* To **share files and memory** via shared volumes

---
Here‚Äôs a **shorter version** of the same explanation, still tailored for an **interview setting** with a bit more emphasis on **AWS and Kubernetes**:

---

### Sidecar Container

In Kubernetes, a **sidecar container** runs alongside the main application container in the same pod to handle supporting tasks. For example, instead of having the backend container push logs‚Äîwhich adds extra load‚Äîwe use a sidecar like **Fluentd** to collect and forward logs.

Fluentd sends logs to **Elasticsearch**, which we use as a centralized logging solution. In our setup, Elasticsearch is managed via **AWS**, enabling scalable and reliable log storage without burdening the application containers.

This approach improves performance, observability, and separation of concerns in our microservices architecture.

---


üìå **Best Practices**:

* Use multi-container pods **only when containers must work together**
* Use **init containers** for setup tasks before main containers start
* Design containers to be loosely coupled and independently replaceable when possible

03.multi-containers.yml
```bash
Kind: pod
apiVersion: v1
metadata: 
    name: multi-container
spec:
    containers:
    - name: nginx
      image: nginx
    - name: alma
      image: "almalinux:9"
      command: ["sleep", "2000"]
```

kubectl apply -f multi-containers.yml

kubectl exec -it <pod-name> -c <container-name> -- bash

---

### 4. **Labels**

**Labels** are **key-value pairs** used to **organize and identify** Kubernetes resources.

---

‚úÖ **Why we use Labels**:

* To group resources logically (e.g., `app=frontend`, `env=prod`)
* To **select resources** for services, deployments, and policies
* To filter and manage resources using `kubectl` or tools like Helm

---

üìå **Best Practices**:

* Define consistent label naming conventions (e.g., `app`, `tier`, `version`)
* Use labels to enable **rolling updates**, monitoring, and scaling
* Avoid putting sensitive or unique data in labels

---

### 5. **Annotations**

**Annotations** are also **key-value metadata**, but they store **non-identifying information** about Kubernetes objects.

---

‚úÖ **Why we use Annotations**:

* To attach **descriptive or operational data** (e.g., build info, monitoring configs)
* To support **external tools and integrations** (e.g., Ingress controllers, CI/CD)
* To provide context or configuration **without affecting selection or grouping**


---

üìå **Best Practices**:

* Use annotations for **metadata that shouldn‚Äôt affect behavior**
* Avoid storing sensitive data (use Secrets instead)
* Use standardized annotation keys where possible (e.g., `kubectl.kubernetes.io/last-applied-configuration`)

**sample manifest files check from below github repo**
https://github.com/xaravind/k8s-resources.git

---

### 6. **Environment Variables**

**Environment variables** are used to **inject configuration values** into containers at runtime.

---

‚úÖ **Why we use Environment Variables**:

* To configure applications **without modifying code or images**
* To provide settings like **database URLs, feature flags, or API keys**
* To make containers **portable across environments**

---

üìå **Best Practices**:

* Use them for simple values (e.g., strings, paths, ports)
* Store values in **ConfigMaps or Secrets** and reference them in pods
* Avoid hardcoding credentials‚Äîuse Secrets for sensitive info

---

once you create pod with env variables, log into pod check env

```bash
kubectl exec -it <pod-name> -- bash

root@env-test:/# env
practice=k8s
project=micro-services
practice=k8s
```
---
### 07. **Resources**

In Kubernetes, **resources** refer to the **compute limits and requests** (CPU and memory) assigned to containers within a pod.

---

‚úÖ **Why we use them**:

* To **prevent a container from using too many resources** and affecting others on the same node
* To **schedule containers efficiently** based on available node capacity
* To **enforce stability and fairness** in multi-tenant environments

---

üîß **Types of Resource Settings**:

1. **Requests**

   * Minimum amount of CPU or memory **guaranteed** to the container
   * Kubernetes uses this to decide **where to schedule** the pod
   * If resources are tight, a pod may not be scheduled unless its request can be met

2. **Limits**

   * The **maximum amount** of CPU or memory a container is allowed to use
   * If a container exceeds its memory limit, it will be **terminated (OOMKilled)**
   * CPU overuse can be **throttled**, not killed

---

üß† **How to determine appropriate values**:

* **Start with baseline usage**: Monitor your app locally or in test environments
* Use Kubernetes tools like:

  * `kubectl top pod` (CPU/memory usage)
  * Metrics server, Prometheus, or Datadog
* Gradually tune **requests and limits** based on observed behavior
* Set conservative defaults, then adjust as needed to avoid underutilization or crashes

---
### 08. **ConfigMaps**

A **ConfigMap** is a Kubernetes object used to **store configuration data** (key-value pairs) separately from application code. This allows you to **inject dynamic configuration** into your containers **without rebuilding images**.

---

‚úÖ **Why we use ConfigMaps**:

* To **externalize app configuration** (e.g., URLs, feature flags, file paths)
* To keep containers **environment-agnostic**
* To **manage changes** to app settings without restarting or redeploying images
* To **reuse configurations** across multiple pods or deployments

---

üß© **Common ways to use ConfigMaps**:

1. **As environment variables** in a container
2. **Mounted as files** inside a container (useful for config files)
3. Accessed programmatically via Kubernetes API (advanced use cases)

---

üìå **Best Practices**:

* Use ConfigMaps for **non-sensitive data** only (use Secrets for sensitive info)
* Combine with **Deployment strategies** to roll out config changes
* Keep your ConfigMaps under version control (e.g., as YAML files in Git)
---

kubectl get configmap # to list configmaps
kubectl describe configmap <configmap-name> # to show the details

### 09. **Secrets**

A **Secret** in Kubernetes is used to store **sensitive data** such as passwords, API keys, tokens, or certificates. Like ConfigMaps, Secrets decouple configuration from your application code ‚Äî but with **added security**.

---

‚úÖ **Why we use Secrets**:

* To **securely store and manage sensitive information**
* To avoid hardcoding secrets in container images or configuration files
* To **inject secrets into pods** via environment variables or mounted volumes
* To support **TLS, authentication, and encrypted credentials** management

---

üîí **Key Features**:

* Secrets are **base64-encoded** in etcd (not encrypted by default, but can be with encryption-at-rest)
* They are only shared with pods that need them
* Kubernetes RBAC controls who can access Secrets

---

üß© **Common use cases**:

* Database credentials
* SSH keys or TLS certificates
* Access tokens for external services (e.g., AWS, GitHub)

---

üìå **Best Practices**:

* Use Secrets **only for confidential data**
* Enable **encryption at rest** for etcd on your cluster
* Limit access with **RBAC policies**
* Avoid printing or logging secrets accidentally
* Use tools like **Sealed Secrets, HashiCorp Vault**, or **external secret stores** for advanced secret management

---

### 10. **Services**

A **Service** is a stable way to expose a set of pods to other services or external users.

---

‚úÖ **Why we use Services**:

* To give pods a **stable DNS name and IP** despite changing pod IPs
* To enable **load balancing** across multiple pods
* To control **how traffic reaches the application**

---

### 10.1 **NodePort**

* Exposes the service on a **static port** on each node's IP
* Accessible from **outside the cluster** via `NodeIP:NodePort`
* ‚ö†Ô∏è Less flexible and secure for production

---

### 10.2 **ClusterIP**

* Default service type
* Exposes the service **internally** within the cluster
* Ideal for **service-to-service** communication

---

### 10.3 **LoadBalancer**

* Provisions an **external load balancer** (cloud-dependent)
* Exposes service to **external traffic** using a **public IP**
* Ideal for **internet-facing applications**

---

### 11. **ReplicaSet**

A **ReplicaSet** ensures that a specified number of **pod replicas** are always running.

---

‚úÖ **Why we use ReplicaSets**:

* To maintain **high availability** and **scalability**
* To **automatically replace failed or terminated pods**
* Forms the foundation for **Deployments**

---

üìå **Best Practices**:

* Don‚Äôt use ReplicaSet directly‚Äîuse **Deployments** which manage ReplicaSets
* Scale up/down using the Deployment for better control
* Monitor pod health and replica status regularly

---
Here's a continuation in the same consistent style for:

---

### 12. **Deployment**

A **Deployment** is a higher-level Kubernetes object that manages ReplicaSets and provides **declarative updates** to applications.

---

‚úÖ **Why we use Deployments**:

* To **automate rollout and rollback** of application versions
* To **scale applications up or down** easily
* To ensure **desired state** by managing ReplicaSets under the hood

---

üìå **Key Features**:

* Supports **rolling updates** to avoid downtime
* Automatically **replaces unhealthy pods**
* Enables **version history tracking and rollback**

---

üìå **Best Practices**:

* Define `strategy` (e.g., `RollingUpdate`) for controlled updates
* Always **version your container images** (avoid `latest`)
* Use probes (`readinessProbe`, `livenessProbe`) for better control over rollout behavior

---

### 13. **StatefulSet**

A **StatefulSet** manages the deployment of **stateful applications**, ensuring each pod has a **stable identity**, **persistent storage**, and **ordered deployment**.

---

‚úÖ **Why we use StatefulSets**:

* For apps that need **stable hostnames** (e.g., databases like MySQL, Cassandra)
* To ensure **ordered startup/shutdown** and consistent storage
* To maintain **sticky identity** across pod restarts

---

üìå **Key Features**:

* Pods get **unique, persistent identities** (`pod-0`, `pod-1`, etc.)
* Supports **volume claim templates** for persistent storage
* Ordered pod deployment and scaling

---

üìå **Best Practices**:

* Use **Headless Services** for network identity
* Combine with **PersistentVolumeClaims** for durable storage
* Avoid using StatefulSets unless app truly requires state

---

### 14. **DaemonSet**

A **DaemonSet** ensures that a **copy of a pod runs on every node** (or a specific group of nodes) in the cluster.

---

‚úÖ **Why we use DaemonSets**:

* For **node-level tasks** like logging, monitoring, or networking agents
* To ensure **system-critical services** run on all nodes
* To deploy pods on **new nodes automatically**

---

üìå **Key Features**:

* Pods are created on **every node** in the cluster
* Automatically schedules pods on **newly added nodes**
* Can target **specific node groups** using selectors

---

üìå **Best Practices**:

* Use for **infrastructure-level workloads**
* Combine with **tolerations** and **nodeSelectors** for targeting
* Avoid scheduling too many resource-heavy pods

---
Here's the continuation in the same format for **Jobs**, **CronJobs**, and **Ingress**:

---

### 15. **Job**

A **Job** creates one or more pods to **run a task to completion**. Once the task finishes successfully, the job is marked as complete.

---

‚úÖ **Why we use Jobs**:

* To run **one-time tasks** like database migrations or batch processing
* To **ensure completion** of critical background operations
* To retry failed tasks a **specified number of times**

---

üìå **Key Features**:

* Automatically **retries failed pods** based on the policy
* Runs until the **success criteria** is met
* Can run **parallel** or **sequential** pods

---

üìå **Best Practices**:

* Set `backoffLimit` to control retries
* Use `ttlSecondsAfterFinished` to clean up old jobs
* Monitor job completion with `kubectl get jobs`

---

### 16. **CronJob**

A **CronJob** runs Jobs **on a time-based schedule**, like Linux cron jobs.

---

‚úÖ **Why we use CronJobs**:

* To run **recurring tasks** (e.g., backups, reports, cleanups)
* For **time-driven automation** in Kubernetes
* To replace external cron-based systems

---

üìå **Key Features**:

* Uses standard cron syntax (`* * * * *`)
* Automatically creates Jobs on schedule
* Supports `startingDeadlineSeconds`, `concurrencyPolicy`, etc.

---

üìå **Best Practices**:

* Set a `successfulJobsHistoryLimit` to avoid clutter
* Use `concurrencyPolicy: Forbid` or `Replace` for overlapping runs
* Avoid over-scheduling ‚Äî ensure the job completes before the next run

---

### 17. **Ingress**

An **Ingress** is a Kubernetes object that manages **external access to services**, typically HTTP/HTTPS, using **rules**.

---

‚úÖ **Why we use Ingress**:

* To expose multiple services under a **single IP or domain**
* To provide **path-based** or **host-based** routing
* To apply **SSL/TLS termination** and **authentication**

---

üìå **Key Features**:

* Uses an **Ingress Controller** (e.g., NGINX, Traefik)
* Supports advanced **routing, redirects, and rewrites**
* Integrates with **TLS certificates** for HTTPS

---

üìå **Best Practices**:

* Always install a reliable Ingress Controller
* Secure routes with TLS and authentication
* Use annotations for custom behavior (timeouts, rewrites, etc.)

---
### 18. **NetworkPolicy**

A **NetworkPolicy** defines how **pods communicate** with each other and with external endpoints at the **network level**.

---

‚úÖ **Why we use NetworkPolicies**:

* To **secure pod-to-pod traffic**
* To restrict access to/from **namespaces, IPs, or ports**
* To enforce **zero-trust networking** in Kubernetes

---

üìå **Key Features**:

* Controls **inbound and outbound traffic**
* Supports rules based on **pod labels, namespaces, IP blocks**
* Works only if a **network plugin** (like Calico or Cilium) supports it

---

üìå **Best Practices**:

* Start with **default deny all** policy
* Apply **least privilege** communication rules
* Document traffic flow to avoid accidental blocking

---

### 19. **Custom Resource (CR)**

A **Custom Resource (CR)** is an extension of Kubernetes API that allows you to **define and manage your own object types**.

---

‚úÖ **Why we use Custom Resources**:

* To support **domain-specific workflows** (e.g., databases, ML pipelines)
* To add **new behavior or automation** with Kubernetes-native integration
* To build **custom controllers** for managing CRs

---

üìå **Key Features**:

* Defined via **CustomResourceDefinition (CRD)**
* Managed like native Kubernetes objects (`kubectl get <crd>`)
* Can be paired with a **Controller** (operator pattern)

---

üìå **Best Practices**:

* Follow Kubernetes object structure for compatibility
* Use CRDs only when core resources aren't sufficient
* Validate CRs with **OpenAPI schemas** in CRD definitions

---







